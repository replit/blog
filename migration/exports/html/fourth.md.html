<blockquote>
<p>The <strong>technological singularity</strong>‚Äîor simply the singularity‚Äîis a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization. According to the most popular version of the singularity hypothesis, I.J. Good&#39;s intelligence explosion model, an upgradable intelligent agent will eventually enter a &quot;runaway reaction&quot; of self-improvement cycles, each new and more intelligent generation appearing more and more rapidly, causing an &quot;explosion&quot; in intelligence and resulting in a powerful superintelligence that qualitatively far surpasses all human intelligence. ‚Äî Wikipedia</p>
</blockquote>
<p>In normieland, where I still spend plenty of time both online and IRL, software‚Äôs newfound ability to write, draw, and speak like we humans is often taken as evidence that the machines are about to remake our entire society (again) and totally change the nature and value of labor (again). I don‚Äôt think the normies are wrong about this, but as flashy as <a href="https://www.jonstokes.com/p/getting-started-with-stable-diffusion">Stable Diffusion</a> and <a href="https://openai.com/blog/chatgpt/">ChatGPT</a> are, old heads know that the Robot Apocalypse has exactly <a href="https://www.jonstokes.com/p/when-software-writes-the-software">one and only one horseman</a>: <strong>computer programs that can write computer programs</strong>.</p>
<p>Artificially intelligent machines can‚Äôt ascend to godhood via the long-prophesied runaway spiral of continuous self-improvement until we invent a machine learning model that can code a better version of itself. We‚Äôre not there yet, and it‚Äôs not clear how far away such a development is, but as of the mid-2021 release of <a href="https://openai.com/blog/openai-codex/">OpenAI‚Äôs Codex model</a>, we‚Äôre a lot closer than we were just a few years ago.</p>
<p>Given that self-improving computer programs would likely be the most important human invention since writing, there is no other part of the AI content generation revolution that‚Äôs worthy of more study and careful scrutiny than generative models that output code. </p>
<p>And of all the new ML-powered programming offerings in this growing ecosystem, Replit has the tools I‚Äôm watching most closely. No other programming platform is in a position to train models on a dataset that includes the following from legions of programmers and millions of projects:</p>
<ul>
<li>Real-time keystroke and clickstream data</li>
<li>Detailed execution and performance data </li>
<li>Character-level file changes</li>
</ul>
<p>If you were going to design a platform for the express purpose of teaching machines to code, it would probably be a cloud-hosted IDE plus execution environment that looks a lot like Replit. So while most of what you‚Äôll read in this article and followups is applicable to all AI code generation tools more generally, I‚Äôll be focusing on Replit‚Äôs toolset because right now it‚Äôs the richest and most advanced, and has the most potential for advancing the state-of-the-art.</p>
<p>In this article series, I‚Äôll walk you through the following topics:</p>
<ol>
<li>What ML code generation can do right now.</li>
<li>Types of programming work this tech might be used for in the near-to-medium term.</li>
<li>The possible (non-singularity) economic, technical, and cultural implications of the widespread adoption of ML in software development.</li>
</ol>
<p>Who these articles are for, in order of focus:</p>
<ul>
<li>New programmers who are just getting started and don‚Äôt have a grasp on the history of the field and how it has evolved to the present point.</li>
<li>Non-programmers who are curious about what this tech can and can‚Äôt do.</li>
<li>Experienced programmers who haven‚Äôt yet tried any of the new ML-powered coding tools and want to understand what they offer both now and in the future.</li>
</ul>
<h1>The five eras of programming</h1>
<p>We all know that programming started back in the olden days with punch cards and people flipping levers and whatever other kind of stuff you can go watch a Ken Burns documentary about. While I give honor to all those who labored in sterile labs and offices at the dawn of the computing era and were more fabulously dressed than the Cheeto-fingered nerds who eventually followed them, I‚Äôd like to start my own discussion of the history of computing with the era we‚Äôre just now emerging out of, which is #3 on the list below:</p>
<p><strong>Programming eras:</strong></p>
<ol>
<li>Academics and engineers with mainframes and minicomputers</li>
<li>Cowboy coders with personal computers</li>
<li>Social coding + cloud + specialization</li>
<li>ML-powered pair programming</li>
<li>ü§ØüöÄü§ñ</li>
</ol>
<p>I came up as a programmer in the PC-powered golden age of the cowboy coder ‚Äî the hero hacker who authored the first version of some piece of software that changed the world. I eventually made the transition to the second era, which featured the rise of GitHub, the adoption of agile programming practices, and all the stuff that made programming more amenable to ‚Äúday job‚Äù treatment. </p>
<p>It was during this time that a kind of <a href="https://en.wikipedia.org/wiki/Charismatic_authority#:~:text=Routinizing%20charisma,-Charismatic%20authority%20almost&text=It%20tends%20to%20challenge%20this,this%20happens%20is%20called%20routinization.">Weberian routinization of charisma</a> took place but in the realm of coding. Success in software projects steadily shifted from being about the unique genius of cowboy coders to being about communication and collaboration among a group of competent professionals of varied backgrounds and skill levels. This shift had three key enabling components:</p>
<ul>
<li><strong>Social coding</strong> meant that your code needed to be readable, testable, and documented. You‚Äôre not just coding for some specialized system, but you‚Äôre also coding for other humans who may come to a project after you, or who you may be sharing solutions in venues like StackOverflow.</li>
<li><strong>Cloud</strong> meant that a lot of development work was about interacting with APIs. A modern web app became a kind of meeting place where multiple APIs for different SaaS products could be mixed and matched underneath layers of business logic and front-end code.</li>
<li><strong>Specialization</strong> meant the rise of back-end engineers vs. front-end engineers or people who were really good at particular kinds of problems (scaling, dealing with storage, front-end optimization, etc.)</li>
</ul>
<p>I lay out these three elements of the third era of programming, which I‚Äôd date from maybe the 2008 launch of GitHub to the 2020 release of OpenAI‚Äôs Codex model in private beta because the fourth age of programming will see each of these areas revolutionized. </p>
<p>Machine learning models have much to offer each of the three parts of the third age, as we‚Äôll see in the following sections.</p>
<h2>Creating a modern web application</h2>
<p>On a practical level, creating a modern web application involves a whole lot of the following kinds of work:</p>
<ul>
<li><strong>Looking up how to talk to an API</strong>, either via a language-specific library or a REST API, and then writing code that makes the correct calls in the correct order.</li>
<li><strong>Generating boilerplate and scaffolding</strong> to initialize a new app, configure a new library in an existing app, or expand an existing app. </li>
<li><strong>Understanding code</strong> that was written by someone else, either some example code on StackOverflow or earlier code from the codebase you‚Äôre currently working in.</li>
</ul>
<p>The first item on the list can take up quite a bit of the time of a junior or mid-level programmer because you have to know where to look for and how to read the relevant docs. This search and rapid comprehension work is the kind of thing you can get a lot faster at as you do more of it. At some point, you end up having done so many repetitions of this read-comprehend-retype loop that your instincts will tell you what the correct code should look like in most situations ‚Äì the lookup activity is still necessary, but it‚Äôs mostly about confirming what your instincts already told you and about naming things properly.</p>
<p>Programmers are also constantly looking up and retyping the correct command-line incantations to get the particular permutation of boilerplate or scaffolding required for your app or feature. As with APIs and libraries, you can expect significant speedups with practice.</p>
<p><img src="https://blog.replit.com/images/fourth/rails-g-scaffold.gif" alt="Generating scaffolding"></p>
<p>Finally, reading and understanding someone else‚Äôs code is also a matter of gaining fluency with a specific skill that gets better with practice. You‚Äôll do a lot of this when you work on an existing application, but you‚Äôll also do it constantly when writing new code because you‚Äôll need to learn from reading code samples and poking around in test suites for popular libraries and APIs.</p>
<p>None of these kinds of labor require enormous mental horsepower. Smarter people will improve at these practiceable skills more rapidly than the less smart, but everyone is going to end up at more-or-less the same place, in the end.</p>
<p>We can reframe the three types of work mentioned above in terms of types of mental activity, in order to make it easier to map onto machine learning‚Äôs capabilities:</p>
<ol>
<li><strong>Reading</strong> and rereading code, docs, forums, and StackOverflow, so as to mentally buffer enough of the latest version of all of it to do steps 2 and 3 below.</li>
<li><strong>Pattern-matching</strong> new programming problems with previously read problem/solution pairs. </li>
<li><strong>Retyping</strong>, in slightly tweaked form, some (pattern-matched) parts of what you‚Äôve read to fit your current programming problem.</li>
</ol>
<p>Ultimately, there are many of us ‚Äî I‚Äôve very much been there as a contract programmer and a startup CTO ‚Äî for whom much of the work of software development is dominated by those three activities. It‚Äôs fortunate, then, that there‚Äôs a natural fit in the above activities for generative machine learning models, where the models are <strong>trained</strong> on large volumes of existing text, and can then do <strong>pattern-matching</strong> (aka ‚Äúinference‚Äù) on novel inputs in order to do guided <strong>generation</strong> of some transformed, context-appropriate version of the training data.</p>
<p>To revisit some concepts from my <a href="https://www.jonstokes.com/p/ai-content-generation-part-1-machine">intro to generative AI</a>, we can now make models where the <strong>latent space</strong> maps to possible computer programs (instead of it mapping to possible images, strings of natural-language text, sounds, etc.). So by searching latent space via a prompt or some other input, we can uncover interesting and useful blocks of code that make computers do things we want them to do.</p>
<p><img src="https://blog.replit.com/images/fourth/latent.png" alt="Latent space"></p>
<p>For another way into this concept, take a statement that was popular in the early days of open-source software, and is currently making a comeback in web3: ‚Äúcode is speech.‚Äù To the extent that code is speech, then any model that outputs useful, meaningful, structured chunks of language that humans can interpret can also be made to output useful, meaningful, structured chunks of language that machines can interpret as commands.</p>
<p>This sounds like fantasy, but it‚Äôs actually being done right now. The results are real, and they‚Äôre spectacular.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Nice read on reverse engineering of GitHub Copilot ü™Ñ. Copilot has dramatically accelerated my coding, it&#39;s hard to imagine going back to &quot;manual coding&quot;. Still learning to use it but it already writes ~80% of my code, ~80% accuracy. I don&#39;t even really code, I prompt. &amp; edit. <a href="https://t.co/kvQTOex9Qj">https://t.co/kvQTOex9Qj</a></p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1608895189078380544?ref_src=twsrc%5Etfw">December 30, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2>Coding with Replit and the current state-of-the-art</h2>
<p>The cloud-hosted programming platform Replit has an excellent, advanced set of ML-powered tools that can substantially reduce the time programmers spend reading, pattern-matching, and re-typing.</p>
<p>There are four main tools that are publicly available right now as part of Replit‚Äôs Ghostwriter suite of ML-powered programming tools: <strong>generate</strong>, <strong>explain</strong>, <strong>transform</strong>, and <strong>autocomplete</strong> code. </p>
<p>If you&#39;ve used the popular Stable Diffusion AI image generator or have read my <a href="https://www.jonstokes.com/p/getting-started-with-stable-diffusion">in-depth introduction</a> to it, each of Replit&#39;s ML-assisted programming tools has a direct analog to what you can do with that image generator:</p>
<ol>
<li><strong>Generate</strong> is the code analog of the standard <code>text2image</code> operation we&#39;re all familiar with, where a text prompt gives you a file from a location in latent space.</li>
<li><strong>Explain</strong> works a little bit like Stable Diffusion&#39;s image-to-text capability, where you can feed it a generated image and a seed and get back the corresponding text prompt. This feature isn&#39;t widely known since it&#39;s not supported in any of the GUI tools, but the model can do it. More generally, you can use many text-to-image models in reverse to generate a textual description of the contents of the image.</li>
<li><strong>Transform</strong> is the code equivalent of Stable Diffusion&#39;s <code>img2img</code> feature, where you&#39;re transforming one input image into another with guidance from a text prompt.</li>
<li><strong>Autocomplete</strong> is also like <code>img2img</code> but without the explict prompt guidance.</li>
</ol>
<p>Let‚Äôs look at each of these in more detail.</p>
<h3>Generate code</h3>
<p>As I said above, Replit&#39;s ‚Äúgenerate code‚Äù tool is very much like what I described in this <a href="https://www.jonstokes.com/p/getting-started-with-stable-diffusion">intro to Stable Diffusion</a>, but instead of turning a text prompt into an image, it turns a text prompt into a program in whatever language you‚Äôre using in the file you‚Äôre editing.</p>
<p><img src="https://blog.replit.com/images/fourth/6_generate_python.mp4" alt="Generate Python"></p>
<p>The main thing this feature does for coders is it lets them tap into the model‚Äôs memory (latent space) for code boilerplate and scaffolding, instead of tapping into their own memory.</p>
<p>Instead of going out and reading all the docs for, say, writing a Discord bot in JavaScript, and then buffering that information in my own brain (or on my computer‚Äôs clipboard) so that I can tweak it to fit my needs, I just pull the starter code from the model‚Äôs memory. Once I get the code into my editor, I can do the last bits of tweaking and transforming it, myself.</p>
<p>What ‚ÄúGenerate Code‚Äù does, then, is to turn this:</p>
<p><img src="https://blog.replit.com/images/fourth/search-code.png" alt="Generating code via Google and copy/paste"></p>
<p>Into this:</p>
<p><img src="https://blog.replit.com/images/fourth/gen-code.png" alt="Generating code on Replit"></p>
<p>As I described in my <a href="https://www.jonstokes.com/p/ai-content-generation-part-1-machine">intro to AI content generation</a>, an input prompt for an ML model is essentially a search query, so I‚Äôm still turning queries into code just like my non-AI-assisted programming peers ‚Äî I‚Äôm just doing it faster and more directly from right within the editor. </p>
<h3>Explain code</h3>
<p>Replit‚Äôs code explanation feature gives you a short text description of what a highlighted block of code does. </p>
<p><img src="https://blog.replit.com/images/fourth/10_learning_explain.mp4" alt="Explain code"></p>
<p>In its present state, this tool strikes me as most useful for programmers who are new to a language and trying to grasp the basic mechanics of what an undocumented function is doing.</p>
<p>However, what I usually want to know when looking at a piece of code is not ‚Äúwhat‚Äôs happening?‚Äù but ‚Äúwhy is this happening?‚Äù And the answer to that ‚Äúwhy‚Äù question always involves some critical context that‚Äôs present in some other file of the application.</p>
<p>In other words, instead of an English-language walkthrough of what a block of code does when it executes, I typically want an explanation that sounds more like, ‚ÄúThis is an event that‚Äôs loaded on the page is loaded and fires when the window scrolls down past the target div, so that the next page of items can be retrieved from the database and inserted into the page. The function retrieves the current page from the DOM, sends a query to the server using the current page and current query string, and then calls a function to render the new items with the results from the query.‚Äù</p>
<p>This kind of higher-level description, where it‚Äôs explaining both the ‚Äúwhat‚Äù and the ‚Äúwhy‚Äù of a function within the specific context of the application, seems very likely to be within the reach of a tool like this, but it‚Äôs going to take some doing. Right now, there‚Äôs just no way to get the amount of context necessary for such an explanation into a pre-trained model ‚Äî input token windows on current models aren‚Äôt nearly large enough to fit a meaningful portion of a real application‚Äôs code base.</p>
<p>To make the code explanations truly a substitute for pairing with a human programmer who deeply understands the code you‚Äôre looking at, my guess is we‚Äôll need models with token windows that are at least in the hundreds of thousands, if not the millions. </p>
<h3>Transform code</h3>
<p>This tool allows you to tweak existing blocks of code by renaming variables, wrapping HTML in tags, wrapping JavaScript functions in a promise, converting older JavaScript dialects to something more modern, or other types of tweaking and cleanup.</p>
<p><img src="https://blog.replit.com/images/fourth/11_learning_modern_js.mp4" alt="Convert to modern JS"></p>
<p>You can even use this tool to translate code from one language to another, which for some of us is even more useful than having it explained in English. For instance, I‚Äôm an advanced rubyist but a python newb, so this language translation ability is helpful for me when I‚Äôm messing with ML-related Jupyter notebooks on Google Colab.</p>
<h3>Autocomplete</h3>
<p>Replit‚Äôs autocomplete tool uses the code and comments you‚Äôve already written to infer what code should go next, and then suggests that code for you.</p>
<p><img src="https://blog.replit.com/images/fourth/gwdemo30.mp4" alt="Ghostwriter"></p>
<p>The relevant Stable Diffusion analogy here is image2image, where the model is taking the text that‚Äôs already in a file, and as you type it‚Äôs doing a live search of latent space for code that‚Äôs adjacent to what you‚Äôve typed and what you‚Äôre typing.</p>
<p>Right now, there are a few limitations to this tool that are related to the fact that its suggestions consist of the output of a search of latent space done via a limited input token window:</p>
<ul>
<li>The tool doesn‚Äôt automatically infer the correct variable names, so when you accept the autocompletion you‚Äôll have to change those, yourself.</li>
<li>Current code suggestion tools, which are based on documentation, are already pretty good, so the performance delta between those and ML-powered autocomplete isn‚Äôt always as noticeable right now as it will be in the future.</li>
</ul>
<p>Again, a much wider input token window will help with both of the above, </p>
<h1>Conclusions</h1>
<p>The current generation of generative coding models is searching latent space for the programming equivalent of paragraphs of text or pictures ‚Äî there‚Äôs a kind of boundedness here, where the models discover discrete code objects that fits specific contexts.</p>
<p>Contrast this to what we eventually want them to do, which is to reason their way from a problem to a solution by conceiving of and then implementing in code a sequence of steps that follow one from the other.</p>
<p>In other words, currently, it‚Äôs up to you as a developer to game out the steps involved in solving a problem ‚Äî when it comes to implementing each step, you can search latent space (instead of searching Google) to find the right block of code. But latent space isn‚Äôt (yet) going to give you the steps, themselves. That‚Äôs because causality is still a hard problem in ML and it‚Äôs still difficult to get models that can reason capably about cause/effect relationships in order to solve completely novel problems. </p>
<p>That said, there have been some successful attempts to get LLMs to generate sequences of steps or ideas based on past training inputs, so there‚Äôs still plenty of coding magic to be uncovered within the existing LLM paradigm that powers the current generation of tools.</p>
<p>In the next article, we‚Äôll take a look at the lifecycle of an application with a view to imagining the kinds of things ML can do for programmers in the near- to medium-term.</p>
<p><img src="https://blog.replit.com/images/fourth/ghost.gif" alt="Generating scaffolding"></p>
