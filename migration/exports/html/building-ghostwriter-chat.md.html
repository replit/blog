<p><img src="https://blog.replit.com/images/Ghostwriter/GW_Header_Image_Final.png" alt="Header Image"></p>
<h2>Why did we build it?</h2>
<p>At Replit, we want to give people the most powerful programming environment, and what better way is there than giving people access to a pair programmer directly in their IDE? Enter Ghostwriter Chat. </p>
<p>Gone are the days where you had to search Stack Overflow for an obscure error message or visit the docs of your favorite package for the millionth time because you forgot what that one argument was called. Interacting with Ghostwriter should be as easy as interacting with a team member, and it is. Since Ghostwriter Chat can have access to your repl file and context,  Ghostwriter can help answer questions about your program without copying and pasting entire code blocks.</p>
<p>We started working on Ghostwriter Chat during our Hackweek in January, and we wanted to be the first to market with an LLM chat application native to your editor. From start to finish, we shipped the product in ~6-8 weeks</p>
<p><img src="https://blog.replit.com/images/gw-example.png" alt="Ghostwriter Chat Example"></p>
<h2>Large Language Models</h2>
<p><a href="https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/">Large language models</a> (LLMs) are the underlying technology powering Ghostwriter Chat. The recent advancements in the field make these language models very good at answering programming questions. </p>
<p>A typical chat request goes through the following steps:</p>
<p><img src="https://blog.replit.com/images/gw_chat_process.png" alt="Ghostwriter Chat Request"></p>
<p>The LLM is the contributor to the largest amount of latency in this process, so streaming the results back instead of waiting for the entire completion is important. If we do not stream, a user will have to wait multiple seconds for the language model to generate all tokens. Streaming gives us the ability to have sub 500 milliseconds from the time a request is sent, to when the user starts seeing a response displayed. </p>
<h2>Generic Prompt Construction</h2>
<p>When working with LLMs, you need to invest a lot of thought into how you construct the prompt that you feed in. A carefully constructed prompt with sufficient context, can yield a very detailed response while a simple, basic prompt can yield something that is not very helpful. </p>
<p>For Ghostwriter to be able to help you answer your questions, it needs to have knowledge of the ever changing environment of your repl and your chat history. To succeed, we needed a way to take varying information from different sources, choose what to include and exclude, and convert it to any number of prompt formats to support any number of models and APIs.</p>
<p><img src="https://blog.replit.com/images/gw-prompt-construction.png" alt="Ghostwriter Chat Prompt Construction"></p>
<h2>Ghostwriter Debugger</h2>
<p>For Ghostwriter Debugger to work, we also need to support the many different ways programs display and throw errors. For example, a running web server can throw errors without pausing execution and continue to output logs, but a script is going to immediately stop with the last output being an error message.</p>
<p><img src="https://blog.replit.com/images/Ghostwriter/debugger-alt-logo-compressed.mp4" alt="Ghostwriter Chat Debugger"></p>
<h2>Getting Around Token Limits</h2>
<p>Unfortunately, language models have a token limit on the input and output tokens. This makes it impossible for us to include all of your repl context, additional external context, and your entire chat history. To get around this, we use some heuristics we feel is a good balance to allow chat to have access to what is most important.</p>
<h4>Limit the chat history we send to the model</h4>
<p>Assuming that the most recent messages are the most important during a conversation, removing old messages is a good way to keep in these context limits.</p>
<h4>Set a cap on how large a message can be</h4>
<p>A message to Ghostwriter counts as part of the prompt and the total limit, so we set a cap at <code>500</code> characters to allow more room for other parts of the prompt (chat history, larger responses, file context). If most users want to reference code they have written in a message and we are including file context already, you can get better results by asking Ghostwriter about the code itself instead of copying and pasting in each message. </p>
<h4>Be smart about what is good context and what is bad context</h4>
<p>For example, a small Repl might only have a hundred or so lines of code and two files. That is very easy to keep the full context of the repl when interacting with Ghostwriter. But what about larger and more complex Repls? It&#39;s impossible to send the entire project to a language model, let alone the entire project and your chat history.</p>
<p>Some files are less relevant than others. A python project might have a directory called <code>.venv/</code>, some linter files, some <code>pytest</code> configurations, a <code>.gitignore</code> file, and a bunch of <code>yaml</code> CI build files. Are these really going to help make Ghostwriter give users better programming advice? Most likely not. The most relevant files would be things like <code>main.py</code> and other source code.</p>
<p>When programming, an example workflow is to think about what you want to do, write some code, forget the best way to do something, look it up, and repeat. This isnt always the case, but if you are focusing programming and you are stuck, it&#39;s almost on the immediate task at hand. We can take advantage of this common flow, by assuming that the most important files for Ghostwriter to know about are the files you are currently working on and have recently worked on. Doing so gives Ghostwriter the most context on the task you are doing at hand and enables it to truly be your pair programmer.</p>
<h2>The Future</h2>
<p>Ghostwriter Chat will only get more and more powerful as AI and LLMs become stronger. Token limits will get larger, inference speeds will get faster, which means future versions of Ghostwriter, and other LLM based applications, will only get smarter, better, and faster. </p>
<p>These trends will allow us to unlock more seamless interaction with our IDE, leverage more file context, and allow Ghostwriter to do more complex actions. Ghostwriter will continue to grow and innovate.</p>
