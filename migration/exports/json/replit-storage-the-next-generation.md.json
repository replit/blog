{"_id":"post-635fab79-2257-4b29-979f-883fa072a796","_type":"post","title":"Replit Storage: The Next Generation","slug":{"_type":"slug","current":"replit-storage-the-next-generation"},"body":[{"_key":"424e628499bb","children":[{"_type":"span","marks":["em"],"text":"Repls today allow 256+ GiB of storage space, up from a historical 1GiB limit. This post is about the infrastructure that enabled that change.","_key":"424e628499bb0"}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"0e430bf20201","children":[{"_type":"span","marks":[],"text":"\n","_key":"0e430bf202010"}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"c736c5b229fe","children":[{"_type":"span","marks":[],"text":"Historically Replit limited files in each Repl to ","_key":"c736c5b229fe0"},{"_type":"span","marks":["19dced19a2b9"],"text":"1 GiB of storage space","_key":"c736c5b229fe1"},{"_type":"span","marks":[],"text":". The 1 GiB limit was enough for small projects, but with the advent of AI model development and with some projects needing gigabytes worth of dependencies to be installed, 1 GiB is not enough. We ","_key":"c736c5b229fe2"},{"_type":"span","marks":["0714b57347fd"],"text":"announced on Developer Day","_key":"c736c5b229fe3"},{"_type":"span","marks":[],"text":" that we were rolling out the infrastructure to unlock much larger Repls, and this is the first part of that. Read on to see why that limit existed in the first place, what changed, and how we pulled it off.","_key":"c736c5b229fe4"}],"markDefs":[{"_key":"19dced19a2b9","_type":"link","href":"https://en.wikipedia.org/wiki/Byte#Multiple-byte_units"},{"_key":"0714b57347fd","_type":"link","href":"https://blog.replit.com/replit-developer-day-recap"}],"_type":"block","style":"normal"},{"_key":"08bf9f0ff569","children":[{"_type":"span","marks":[],"text":"How Repls Previously Operated","_key":"08bf9f0ff5690"}],"markDefs":[],"_type":"block","style":"h2"},{"_key":"1b04ae18f868","children":[{"_type":"span","_key":"1b04ae18f8680","text":"","marks":[]}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"4229e91f4f57","children":[{"_type":"span","marks":[],"text":"All Repls use ","_key":"4229e91f4f570"},{"_type":"span","marks":["222845d12507"],"text":"btrfs","_key":"4229e91f4f571"},{"_type":"span","marks":[],"text":" as their filesystem of choice. Back in 2018 when this decision was made, btrfs was already a very stable filesystem (and even some Linux distributions, ","_key":"4229e91f4f572"},{"_type":"span","marks":["89805e8cb5d5"],"text":"like Fedora, later adopted it by default","_key":"4229e91f4f573"},{"_type":"span","marks":[],"text":"). Aside from its stability / performance considerations, the two features we really liked about btrfs were that it supported setting quotas (to enforce the 1 GiB per-Repl limit) and that it supported ","_key":"4229e91f4f574"},{"_type":"span","marks":["b23ac510d501"],"text":"copy-on-write snapshots","_key":"4229e91f4f575"},{"_type":"span","marks":[],"text":" as the building blocks for storage.","_key":"4229e91f4f576"}],"markDefs":[{"_key":"222845d12507","_type":"link","href":"https://en.wikipedia.org/wiki/Btrfs"},{"_key":"89805e8cb5d5","_type":"link","href":"https://fedoraproject.org/wiki/Changes/BtrfsByDefault"},{"_key":"b23ac510d501","_type":"link","href":"https://fedoramagazine.org/working-with-btrfs-snapshots/"}],"_type":"block","style":"normal"},{"_key":"e0ac0d82f212","children":[{"_type":"span","marks":[],"text":"That takes care of the filesystem aspect, but how were the filesystems provisioned? Since we run several Repls in each actual machine, the provisioning of block devices was done through ","_key":"e0ac0d82f2120"},{"_type":"span","marks":["e406d00149a3"],"text":"LVM thin pools","_key":"e0ac0d82f2121"},{"_type":"span","marks":[],"text":", which allows having a fixed-size physical block device and creates an arbitrary number of virtual block devices, as long as the total used size of all the block devices doesn’t exceed the capacity of the physical block device.","_key":"e0ac0d82f2122"}],"markDefs":[{"_key":"e406d00149a3","_type":"link","href":"https://wiki.gentoo.org/wiki/LVM/en#Thin_provisioning"}],"_type":"block","style":"normal"},{"_key":"8a7d1655e45c","children":[{"_type":"span","marks":[],"text":"These two building blocks came together to provision Repls' filesystems: every time a new Repl started, we asked the LVM thin pool for two 2 GiB devices, format them both as btrfs, set a 1 GiB quota on one, and then do very complicated mounts with ","_key":"8a7d1655e45c0"},{"_type":"span","marks":["76b685c85e53","code"],"text":"overlayfs","_key":"8a7d1655e45c1"},{"_type":"span","marks":[],"text":": The device with 1 GiB quota becomes the Repls' filesystem (mounted at ","_key":"8a7d1655e45c2"},{"_type":"span","marks":["code"],"text":"/home/runner/${REPL_SLUG}","_key":"8a7d1655e45c3"},{"_type":"span","marks":[],"text":") and the other one (known as the \"scratch\" disk) becomes the upper dir for every other directory in the system that is writable (like ","_key":"8a7d1655e45c4"},{"_type":"span","marks":["code"],"text":"/home/runner","_key":"8a7d1655e45c5"},{"_type":"span","marks":[],"text":", ","_key":"8a7d1655e45c6"},{"_type":"span","marks":["code"],"text":"/nix","_key":"8a7d1655e45c7"},{"_type":"span","marks":[],"text":", ","_key":"8a7d1655e45c8"},{"_type":"span","marks":["code"],"text":"/tmp","_key":"8a7d1655e45c9"},{"_type":"span","marks":[],"text":", etc.). Once the Repl's filesystem is mounted, we streamed a ","_key":"8a7d1655e45c10"},{"_type":"span","marks":["d1dd6a2b7f4a"],"text":"serialized version","_key":"8a7d1655e45c11"},{"_type":"span","marks":[],"text":" of the last snapshot of the filesystem, and then the Repl was ready to go!","_key":"8a7d1655e45c12"}],"markDefs":[{"_key":"76b685c85e53","_type":"link","href":"https://docs.kernel.org/filesystems/overlayfs.html"},{"_key":"d1dd6a2b7f4a","_type":"link","href":"https://btrfs.readthedocs.io/en/latest/btrfs-receive.html"}],"_type":"block","style":"normal"},{"_key":"524a330dbd7e","children":[{"_type":"span","marks":[],"text":"All this process was slower and more brittle the larger the Repl was. And that was the reason why Repls had been historically capped to 1 GiB: anything else was way too much for this system to handle, since folks would be waiting ","_key":"524a330dbd7e0"},{"_type":"span","marks":["em"],"text":"several minutes","_key":"524a330dbd7e1"},{"_type":"span","marks":[],"text":" for the filesystem to be deserialized, even with clever compression techniques and pooling thousands of ready-to-go pre-formatted virtual LVM devices. That was a terrible experience for Repls larger than a certain size, especially if there was ","_key":"524a330dbd7e2"},{"_type":"span","marks":["em"],"text":"any","_key":"524a330dbd7e3"},{"_type":"span","marks":[],"text":" problem with the Repl, since it would need several minutes to recover (or worse: several minutes to crashloop). In fact, we tried using this technique to ","_key":"524a330dbd7e4"},{"_type":"span","marks":["1921284585a4"],"text":"develop the first proof-of-concept for Replit on Replit","_key":"524a330dbd7e5"},{"_type":"span","marks":[],"text":" several months ago, and while it was a neat experiment, it was pretty much unusable due to the slow boot times. Side note, this was also extremely expensive: every time ","_key":"524a330dbd7e6"},{"_type":"span","marks":["em"],"text":"any","_key":"524a330dbd7e7"},{"_type":"span","marks":[],"text":" file in a Repl was changed, we needed to do a full snapshot and save the full serialized version to stable storage. We ran a profile of the system, and we were burning around 50% of the CPU just on this process.","_key":"524a330dbd7e8"}],"markDefs":[{"_key":"1921284585a4","_type":"link","href":"https://www.youtube.com/watch?v=7TCqGslll-4&t=1835"}],"_type":"block","style":"normal"},{"_key":"d7f313d6e883","children":[{"_type":"span","marks":[],"text":"I can't believe it's not btr(fs snapshots)","_key":"d7f313d6e8830"}],"markDefs":[],"_type":"block","style":"h2"},{"_key":"b141884f956d","children":[{"_type":"span","_key":"b141884f956d0","text":"","marks":[]}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"809920be4355","children":[{"_type":"span","marks":[],"text":"We knew we had to move away from persisting disks as snapshots. Allowing Repls to grow to unbounded sizes would increase boot time as well. Instead of persisting entire Repls all at once we needed a system that would allow a Repl to access the data it needed on demand, without loading the entire contents of the Repl at boot. Our first prototype of expandable storage gave each Repl its own NFS directory. ","_key":"809920be43550"},{"_type":"span","marks":["ece3523b6cff"],"text":"Network File System","_key":"809920be43551"},{"_type":"span","marks":[],"text":" is a network-first shared filesystem that was designed to solve the problem of sharing a filesystem between many computers across a network. While NFS solved our problem in theory, it quickly developed some cracks.","_key":"809920be43552"}],"markDefs":[{"_key":"ece3523b6cff","_type":"link","href":"https://en.wikipedia.org/wiki/Network_File_System"}],"_type":"block","style":"normal"},{"_key":"c3b9d7afd52b","children":[{"_type":"span","marks":[],"text":"Deciding to move to a network file system was only half the battle. We considered whether migrating everything to a managed NFS service could serve this need, but nothing we could find would be able to scale to the size and performance we needed. We would have to build something bespoke. Because NFS is a filesystem, it is a complex protocol. Permissions, metadata, directory structure, ensuring consistency, and every other filesystem concern has to be handled at the protocol layer. Implementing our own NFS server that could store files to permanent storage (we're using ","_key":"c3b9d7afd52b0"},{"_type":"span","marks":["9051657dfa88"],"text":"Google Cloud Storage","_key":"c3b9d7afd52b1"},{"_type":"span","marks":[],"text":" or GCS for that) could be ","_key":"c3b9d7afd52b2"},{"_type":"span","marks":["em"],"text":"very","_key":"c3b9d7afd52b3"},{"_type":"span","marks":[],"text":" error prone, and once we built it we would need to write to a GCS file every time anyone touched a file in a repl, which would be cost prohibitive and high latency. Between the amount of effort that building a safe NFS service would take, and the risk that making such a service fast enough for the experience in a repl to feel local, we decided we needed to think simpler.","_key":"c3b9d7afd52b4"}],"markDefs":[{"_key":"9051657dfa88","_type":"link","href":"https://cloud.google.com/storage"}],"_type":"block","style":"normal"},{"_key":"e1024122717c","children":[{"_type":"span","marks":[],"text":"The biggest challenge in a network filesystem is ensuring consistency across many clients. Since Repls are only running in one place at a time, that isn't an issue for us. If you remove the multi-writer requirement from a network file system, then it's really no different from a local filesystem. With that in mind we considered leaving the filesystem on the machine running the Repl, and instead moving the disk itself to another machine.","_key":"e1024122717c0"}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"6a66ea2e7aa7","children":[{"_type":"span","marks":[],"text":"There are many protocols to serve a ","_key":"6a66ea2e7aa70"},{"_type":"span","marks":["em","a29a96f94955"],"text":"block device","_key":"6a66ea2e7aa71"},{"_type":"span","marks":[],"text":", something that the kernel sees as a raw disk, over the network: ","_key":"6a66ea2e7aa72"},{"_type":"span","marks":["23433fff0e1c"],"text":"iSCSI","_key":"6a66ea2e7aa73"},{"_type":"span","marks":[],"text":", ","_key":"6a66ea2e7aa74"},{"_type":"span","marks":["09459aec01fc"],"text":"AoE","_key":"6a66ea2e7aa75"},{"_type":"span","marks":[],"text":", various flavors of ","_key":"6a66ea2e7aa76"},{"_type":"span","marks":["e7537a3c9ae6"],"text":"fiber channel","_key":"6a66ea2e7aa77"},{"_type":"span","marks":[],"text":", ","_key":"6a66ea2e7aa78"},{"_type":"span","marks":["fe2e6edd237b"],"text":"NBD","_key":"6a66ea2e7aa79"},{"_type":"span","marks":[],"text":". All of them allow a remote machine to provide a disk that can be accessed just like any other disk attached to the machine. That would enable us to continue using btrfs as the filesystem for a repl, and not pay any of the performance overhead of moving the entire filesystem to the network, while still allowing us to store the bytes of a repl off the machine. And it simplifies the persistence problem from storing thousands of separate files with their own metadata and permissions to storing a big chunk of bytes.","_key":"6a66ea2e7aa710"}],"markDefs":[{"_key":"a29a96f94955","_type":"link","href":"https://en.wikipedia.org/wiki/Device_file#Block_devices"},{"_key":"23433fff0e1c","_type":"link","href":"https://en.wikipedia.org/wiki/ISCSI"},{"_key":"09459aec01fc","_type":"link","href":"https://en.wikipedia.org/wiki/ATA_over_Ethernet"},{"_key":"e7537a3c9ae6","_type":"link","href":"https://en.wikipedia.org/wiki/Fibre_Channel_over_Ethernet"},{"_key":"fe2e6edd237b","_type":"link","href":"https://en.wikipedia.org/wiki/Network_block_device"}],"_type":"block","style":"normal"},{"_key":"d81f53b4a0c4","children":[{"_type":"span","marks":[],"text":"While there are many existing servers for serving disks over the network, most are tailored to the needs of ","_key":"d81f53b4a0c40"},{"_type":"span","marks":["97524261348a"],"text":"SANs","_key":"d81f53b4a0c41"},{"_type":"span","marks":[],"text":", where exports are configured statically and disks are served to a fixed number of long-lived clients. In order to store to GCS, we needed to build something ourselves. That service we called margarine","_key":"d81f53b4a0c42"},{"_type":"span","marks":["948ac6ecb942"],"text":"[1]","_key":"d81f53b4a0c43"},{"_type":"span","marks":[],"text":", and since it needed to be blazing fast, we wrote it in Rust.","_key":"d81f53b4a0c44"}],"markDefs":[{"_key":"97524261348a","_type":"link","href":"https://en.wikipedia.org/wiki/Storage_area_network"},{"_key":"948ac6ecb942","_type":"link","href":"#footnote-1"}],"_type":"block","style":"normal"},{"_key":"ab15b3b4b435","children":[{"_type":"span","marks":[],"text":"How does it work","_key":"ab15b3b4b4350"}],"markDefs":[],"_type":"block","style":"h2"},{"_key":"a0dca68efb41","children":[{"_type":"span","marks":[],"text":"When you boot up a Repl, the machine running it sends a connection request to one of our margarine servers, ","_key":"a0dca68efb410"},{"_type":"span","marks":["ba8e98e30ae6"],"text":"chosen with our own loadbalancer","_key":"a0dca68efb411"},{"_type":"span","marks":[],"text":". The server validates the request and establishes an NBD session with the Repl. That is all that needs to happen before you boot your Repl, meaning Repls can boot instantly now, regardless of how big they are.","_key":"a0dca68efb412"}],"markDefs":[{"_key":"ba8e98e30ae6","_type":"link","href":"https://blog.replit.com/geo-part-2-loadbalancing"}],"_type":"block","style":"normal"},{"_key":"f59576db04ad","children":[{"_type":"span","marks":[],"text":"Of course your Repl is probably going to need to access some files once it boots up. It will try to read things from btrfs, which will start sending requests to read sectors of the NBD disk to the margarine server. Margarine slices up the entire virtual size of the disk into 16MiB blocks and determines which block the requested sectors fall into, downloading that entire block from GCS and writing it to a local scratch disk. The resulting 16MiB from GCS is then sliced up again into 512KiB blocks and copied into memory. Any requests can then be served directly from memory. By transferring 16MiB back and forth from GCS all at once we can take advantage of the fact that file systems try to group relevant data on disk, this allows us to minimize the amount of time we spend page faulting to a high layer of the cache and keep things moving quickly. Many of our templates are small enough to only require downloading one or two files from GCS in order to populate their entire filesystems.","_key":"f59576db04ad0"}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"a128d5c91458","children":[{"_type":"span","marks":[],"text":"As you access data in your Repl, margarine can add and remove things from its cache and move things from disk to memory so that what you need is always fresh, while things that you aren't using anymore can be shuffled back out to GCS until they're needed. The result is a virtual disk that is bounded only by the maximum size of a btrfs filesystem (16EiB ought to be enough for anybody).","_key":"a128d5c914580"}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"f5c426e2d846","children":[{"_type":"span","marks":[],"text":"Transaction-safety","_key":"f5c426e2d8460"}],"markDefs":[],"_type":"block","style":"h3"},{"_key":"83cac817766b","children":[{"_type":"span","marks":[],"text":"In our old implementation, every time there was an explicit readonly snapshot when we wanted to serialize the filesystem, the filesystem had good guarantees of being in a very consistent state. Now we are always sending all filesystem writes over the network, and anything that goes over the network can be disconnected ","_key":"83cac817766b0"},{"_type":"span","marks":["7d9fc4d9abda"],"text":"for all sorts of reasons","_key":"83cac817766b1"},{"_type":"span","marks":[],"text":". This meant that Repls would now be a lot more susceptible to random data corruption. This was enough of a worry that we added ","_key":"83cac817766b2"},{"_type":"span","marks":["334f8734f255"],"text":"chaos testing","_key":"83cac817766b3"},{"_type":"span","marks":[],"text":" tools that injected network failures at random points during the connection by dropping connections using ","_key":"83cac817766b4"},{"_type":"span","marks":["18600dcde2e1","code"],"text":"tcpkill","_key":"83cac817766b5"},{"_type":"span","marks":[],"text":" under multiple workloads and reconnecting, over and over. And we did end up getting corrupted btrfs filesystems within seconds :'(. Of course, the client can try its very best to avoid losing a connection, but the network is mostly out of our control, which meant that we needed to actively prevent this from happening. So we needed to figure out how to a) detect safe points to actually persist the filesystem contents to permanent storage and b) discard anything that was modified since the last safe point if the connection got dropped unexpectedly. All that was left was to understand how btrfs works in the kernel.","_key":"83cac817766b6"}],"markDefs":[{"_key":"7d9fc4d9abda","_type":"link","href":"https://en.wikipedia.org/wiki/Anna_Karenina_principle"},{"_key":"334f8734f255","_type":"link","href":"https://en.wikipedia.org/wiki/Chaos_engineering"},{"_key":"18600dcde2e1","_type":"link","href":"https://linux.die.net/man/8/tcpkill"}],"_type":"block","style":"normal"},{"_key":"0c943dc7e494","children":[{"_type":"span","marks":["5d2dcce6c171"],"text":"Btrfs is a copy-on-write filesystem based on btrees","_key":"0c943dc7e4940"},{"_type":"span","marks":[],"text":", which_ roughly _means that every time there's a change in the data in the system, it creates a copy of the data and all the btrees that reference it, writes all the updated data structures in another region of the device, and then it deletes the original copy when it's no longer needed. But if information is not updated in-place, how does one find the latest state of the system? There is just one exception to that rule: the ","_key":"0c943dc7e4941"},{"_type":"span","marks":["90939aeedf4a"],"text":"super blocks","_key":"0c943dc7e4942"},{"_type":"span","marks":[],"text":". These are in-disk data structures that are located in fixed locations in the device, and have pointers to the \"root btrees\", which hold pointers to all other btrees that store data and metadata. This is an oversimplification of how btrfs works and lays the data on disk. If you want more information, Dxu wrote a very good series of blogposts that talk about ","_key":"0c943dc7e4943"},{"_type":"span","marks":["a92b652ece50"],"text":"btrfs internals","_key":"0c943dc7e4944"},{"_type":"span","marks":[],"text":". Ohad Rodeh also presented some of the concepts used by btrfs in ","_key":"0c943dc7e4945"},{"_type":"span","marks":["64395b89708d"],"text":"B-trees, Shadowing, and Clones","_key":"0c943dc7e4946"},{"_type":"span","marks":[],"text":".","_key":"0c943dc7e4947"}],"markDefs":[{"_key":"5d2dcce6c171","_type":"link","href":"https://archive.kernel.org/oldwiki/btrfs.wiki.kernel.org/index.php/Btrfs_design.html"},{"_key":"90939aeedf4a","_type":"link","href":"https://archive.kernel.org/oldwiki/btrfs.wiki.kernel.org/images-btrfs/d/d7/Design-roots.png"},{"_key":"a92b652ece50","_type":"link","href":"https://dxuuu.xyz/btrfs-internals.html"},{"_key":"64395b89708d","_type":"link","href":"https://archive.kernel.org/oldwiki/btrfs.wiki.kernel.org/images-btrfs/6/68/Btree_TOS.pdf"}],"_type":"block","style":"normal"},{"_key":"ff56b4f174c4","children":[{"_type":"span","marks":[],"text":"Keen observers might realize an important consequence of btrfs' design: the moment after all the super blocks are updated is when the filesystem is fully consistent. All the on-disk data structures are freshly written, and the pointers to these data structures are now updated in the super blocks. And since the super blocks are written to fixed locations, we can detect every time this happens. If we look at the kernel implementation of the point in time in which transactions are finalized, we arrive at ","_key":"ff56b4f174c40"},{"_type":"span","marks":["4846f6c0da24"],"text":"write_all_supers","_key":"ff56b4f174c41"},{"_type":"span","marks":[],"text":". This function ultimately does a sequence of sending a flush request, writing the first super block with the ","_key":"ff56b4f174c42"},{"_type":"span","marks":["27bd547ed4e6"],"text":"Force Unit Access flag (FUA)","_key":"ff56b4f174c43"},{"_type":"span","marks":[],"text":", and then writing the rest of the super blocks. This sequence is pretty easy to detect server-side due to the fixed offsets and sizes of the writes (4 kiB each), and we use it to know which are the safest points to fully persist the contents of the filesystem.","_key":"ff56b4f174c44"}],"markDefs":[{"_key":"4846f6c0da24","_type":"link","href":"https://github.com/torvalds/linux/blob/3d7cb6b0/fs/btrfs/disk-io.c#L4399"},{"_key":"27bd547ed4e6","_type":"link","href":"https://www.kernel.org/doc/Documentation/block/writeback_cache_control.txt"}],"_type":"block","style":"normal"},{"_key":"f5d9776d8275","children":[{"_type":"span","marks":[],"text":"It turned out that completely ignoring the kernel's flush / FUA requests and detecting this sequence made our chaos tester completely happy: no more data corruption!","_key":"f5d9776d82750"},{"_type":"span","marks":["e349e6edfc2d"],"text":"[2]","_key":"f5d9776d82751"},{"_type":"span","marks":[],"text":" This makes our implementation somewhat tightly coupled with the btrfs implementation, but it's the only filesystem we're going to support in the foreseeable future.","_key":"f5d9776d82752"}],"markDefs":[{"_key":"e349e6edfc2d","_type":"link","href":"#footnote-2"}],"_type":"block","style":"normal"},{"_key":"6d19403cbb5a","children":[{"_type":"span","marks":[],"text":"Global copy-on-write","_key":"6d19403cbb5a0"}],"markDefs":[],"_type":"block","style":"h3"},{"_key":"eb37545219b7","children":[{"_type":"span","marks":[],"text":"While writing the transaction-safe functionality we needed to have a way to persist the contents of the filesystem atomically and asynchronously, because now we're persisting potentially hundreds of megabytes at the same time, and having the kernel wait for the filesystem to be fully flushed to persistent storage took way too much time. This now meant that if we happened to fork a Repl mid-transaction, we would end up with an inconsistent view of the filesystem, undoing all the work that we had done before! But if we took a step back, this sounded ","_key":"eb37545219b70"},{"_type":"span","marks":["em"],"text":"a lot","_key":"eb37545219b71"},{"_type":"span","marks":[],"text":" like the same problem that btrfs is solving with their transactions. So ","_key":"eb37545219b72"},{"_type":"span","marks":["em"],"text":"what if","_key":"eb37545219b73"},{"_type":"span","marks":[],"text":" we used the same solution: copy-on-write? It turns out that it worked very well! And as a bonus, it unlocked a few more things.","_key":"eb37545219b74"}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"4b2bda90c78d","children":[{"_type":"span","marks":[],"text":"Every Repl has a small \"manifest\", which is a fancy word for a map of the used blocks that each Repl has. Each block has a UUID assigned to it when it is created, and also every time a block transitions from being clean (its contents exactly match what was persisted to permanent storage) to being dirty. When there is a btrfs commit, we create a copy of this map, update it with pointers to the new blocks, and persist the blocks with their associated UUID, plus the manifest. So as long as the manifest reads are atomic, we can guarantee that every time there is a fork, we are getting a consistent snapshot of the full filesystem. And now that we have copy-on-write for every Repl, we can do something cool: instant and cheaper forks. A fork of a Repl now only needs to copy the manifest, and all the blocks owned by the original Repl will be copy-on-write modified by the fork, which saves on storage. But now that we had forks, we also realized that we gained something we didn't expect: we can have the full filesystem history of a Repl! Saving ","_key":"4b2bda90c78d0"},{"_type":"span","marks":["em"],"text":"every","_key":"4b2bda90c78d1"},{"_type":"span","marks":[],"text":" full snapshot of every Repl would quickly get out of control, so we ended up using Evan Wallace's excellent ","_key":"4b2bda90c78d2"},{"_type":"span","marks":["dad98fef81dd"],"text":"Logarithmically-Spaced Snapshots","_key":"4b2bda90c78d3"},{"_type":"span","marks":[],"text":" algorithm to have a rough upper bound on the storage that each Repl could use in total, plus being able to have multiple versions of the full filesystem for recovery purposes just in case something went terribly wrong, as well as being able to debug some very difficult-to-diagnose bugs that we've seen over the years.","_key":"4b2bda90c78d4"}],"markDefs":[{"_key":"dad98fef81dd","_type":"link","href":"https://madebyevan.com/algos/log-spaced-snapshots/"}],"_type":"block","style":"normal"},{"_key":"e2e101ab8048","children":[{"_type":"span","marks":[],"text":"Finally, the last side-effect of this move: read-only forks are still possible, and cheaper too. We use those every time a user visits a Repl owned by another user. With a little bit of work, we were also able to make this whole scheme ","_key":"e2e101ab80480"},{"_type":"span","marks":["50cb1f0d96bc"],"text":"geographically-aware","_key":"e2e101ab80481"},{"_type":"span","marks":[],"text":" so that users in India could fork Repls from the US and vice versa. So many benefits, there has to be a catch, right? Well, yes: Part of the copy-on-write process is that we need to delete blocks that are no longer being referenced. Now that we have multiple locations where data could be stored, it makes things a bit harder. As it turned out, a very simple ","_key":"e2e101ab80482"},{"_type":"span","marks":["d164fd208a43"],"text":"tracing garbage collection process","_key":"e2e101ab80483"},{"_type":"span","marks":[],"text":", where we scan all manifests to see what blocks they reference, the blocks that are present, and then take the symmetric difference to see what blocks are present but not referenced did the trick, with some additional constraints to accommodate for manifests that were created / modified since the scanning process started. Fully copy-on-write-enabled geographically-aware Repls!","_key":"e2e101ab80484"}],"markDefs":[{"_key":"50cb1f0d96bc","_type":"link","href":"https://blog.replit.com/geo-part-3-deploy"},{"_key":"d164fd208a43","_type":"link","href":"https://en.wikipedia.org/wiki/Tracing_garbage_collection"}],"_type":"block","style":"normal"},{"_key":"ef4eeb252ea7","children":[{"_type":"span","marks":[],"text":"What's next?","_key":"ef4eeb252ea70"}],"markDefs":[],"_type":"block","style":"h2"},{"_key":"682b9bc379fa","children":[{"_type":"span","marks":[],"text":"We have all the necessary infrastructure in place to start the rollout for all Repls to have larger filesystem sizes. Stay tuned for an upcoming blogpost where discuss some of the specifics around enlarging Repls! Expect some news in the next couple of weeks!","_key":"682b9bc379fa0"}],"markDefs":[],"_type":"block","style":"normal"},{"_key":"21e6e1a6c26c","children":[{"_type":"span","marks":[],"text":"We're also in the process of upgrading our production kernels because there is ","_key":"21e6e1a6c26c0"},{"_type":"span","marks":["d452f998beb9"],"text":"one very shiny feature","_key":"21e6e1a6c26c1"},{"_type":"span","marks":[],"text":" that is going to let us do much better detection of safe points and allow the editor to know what versions of the files have been persisted all the way to permanent storage.","_key":"21e6e1a6c26c2"}],"markDefs":[{"_key":"d452f998beb9","_type":"link","href":"https://docs.kernel.org/block/ublk.html"}],"_type":"block","style":"normal"},{"_key":"191809ff6cdc","children":[{"_type":"span","marks":[],"text":"Work at Replit","_key":"191809ff6cdc0"}],"markDefs":[],"_type":"block","style":"h2"},{"_key":"fec8f76982ce","children":[{"_type":"span","marks":[],"text":"Are you interested in Linux and filesystems? What about networking and distributed systems? ","_key":"fec8f76982ce0"},{"_type":"span","marks":["b9d0a5cb6257"],"text":"Come work with us","_key":"fec8f76982ce1"},{"_type":"span","marks":[],"text":" to solve challenging problems and enable the next billion software creators online.","_key":"fec8f76982ce2"}],"markDefs":[{"_key":"b9d0a5cb6257","_type":"link","href":"https://replit.com/site/careers"}],"_type":"block","style":"normal"},{"_key":"9da5c70756cf","children":[{"_type":"span","marks":[],"text":"Footnotes","_key":"9da5c70756cf0"}],"markDefs":[],"_type":"block","style":"h3"},{"_key":"15d77782a3b1","children":[{"_type":"span","marks":[],"text":"Margarine: it's not btr(fs) snapshots, ha ha. We love puns. ","_key":"15d77782a3b10"},{"_type":"span","marks":["e7c299a98941"],"text":"↩","_key":"15d77782a3b11"}],"markDefs":[{"_key":"e7c299a98941","_type":"link","href":"#footnote-1-ref"}],"_type":"block","style":"normal","level":1,"listItem":"number"},{"_key":"c25cda2148c9","children":[{"_type":"span","marks":[],"text":"Turns out that there was one more case when this happened: if there are creations / deletions of btrfs snapshots and the connection is dropped at just the right time, there are some ","_key":"c25cda2148c90"},{"_type":"span","marks":["436a2c8abe40"],"text":"orphans that are not completely cleaned up","_key":"c25cda2148c91"},{"_type":"span","marks":[],"text":". We haven't yet identified how non-cleaned orphans end up corrupting the filesystem after multiple rounds of the chaos tester, but since we stopped needing to do btrfs snapshots altogether, we didn't pursue any further. Do reach out if you're interested in getting a repro scenario to file a bug report, though! ","_key":"c25cda2148c92"},{"_type":"span","marks":["bf0f9abca205"],"text":"↩","_key":"c25cda2148c93"}],"markDefs":[{"_key":"436a2c8abe40","_type":"link","href":"https://github.com/torvalds/linux/blob/3d7cb6b04c3f3115719235cc6866b10326de34cd/fs/btrfs/ioctl.c#L841"},{"_key":"bf0f9abca205","_type":"link","href":"#footnote-2-ref"}],"_type":"block","style":"normal","level":1,"listItem":"number"}],"publishedAt":"2023-07-19T23:00:00.000Z"}